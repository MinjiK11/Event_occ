2024-05-31 09:58:17,328 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.11 (default, Jul 27 2021, 14:32:16) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7,8,9: NVIDIA RTX A5000
CUDA_HOME: /usr/local/cuda
NVCC: Build cuda_11.1.TC455_06.29190527_0
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.9.1+cu111
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.10.1+cu111
OpenCV: 4.9.0
MMCV: 1.4.0
MMCV Compiler: GCC 7.5
MMCV CUDA Compiler: 11.1
MMDetection: 2.14.0
MMSegmentation: 0.14.1
MMDetection3D: 0.17.1+unknown
------------------------------------------------------------

2024-05-31 09:58:18,857 - mmdet - INFO - Distributed training: False
2024-05-31 09:58:20,344 - mmdet - INFO - Config:
checkpoint_config = None
log_config = dict(
    interval=50,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = 'result/voxformer-S'
load_from = None
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
_num_layers_cross_ = 3
_num_points_cross_ = 8
_num_layers_self_ = 2
_num_points_self_ = 8
_dim_ = 128
_pos_dim_ = 64
_ffn_dim_ = 256
_num_levels_ = 1
_labels_tag_ = 'labels'
_num_cams_ = 1
_temporal_ = []
point_cloud_range = [0, -25.6, -2.0, 51.2, 25.6, 4.4]
voxel_size = [0.2, 0.2, 0.2]
_sem_scal_loss_ = True
_geo_scal_loss_ = True
_depthmodel_ = 'msnet3d'
_nsweep_ = 10
_query_tag_ = 'query_iou5203_pre7712_rec6153'
model = dict(
    type='VoxFormer',
    pretrained=dict(img='ckpts/resnet50-19c8e357.pth'),
    img_backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(2, ),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=False),
        norm_eval=True,
        style='pytorch'),
    img_neck=dict(
        type='FPN',
        in_channels=[1024],
        out_channels=128,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=1,
        relu_before_extra_convs=True),
    pts_bbox_head=dict(
        type='VoxFormerHead',
        bev_h=128,
        bev_w=128,
        bev_z=16,
        embed_dims=128,
        CE_ssc_loss=True,
        geo_scal_loss=True,
        sem_scal_loss=True,
        cross_transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            embed_dims=128,
            num_cams=1,
            encoder=dict(
                type='VoxFormerEncoder',
                num_layers=3,
                pc_range=[0, -25.6, -2.0, 51.2, 25.6, 4.4],
                num_points_in_pillar=8,
                return_intermediate=False,
                transformerlayers=dict(
                    type='VoxFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='DeformCrossAttention',
                            pc_range=[0, -25.6, -2.0, 51.2, 25.6, 4.4],
                            num_cams=1,
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=128,
                                num_points=8,
                                num_levels=1),
                            embed_dims=128)
                    ],
                    ffn_cfgs=dict(
                        type='FFN',
                        embed_dims=128,
                        feedforward_channels=1024,
                        num_fcs=2,
                        ffn_drop=0.0,
                        act_cfg=dict(type='ReLU', inplace=True)),
                    feedforward_channels=256,
                    ffn_dropout=0.1,
                    operation_order=('cross_attn', 'norm', 'ffn', 'norm')))),
        self_transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            embed_dims=128,
            num_cams=1,
            encoder=dict(
                type='VoxFormerEncoder',
                num_layers=2,
                pc_range=[0, -25.6, -2.0, 51.2, 25.6, 4.4],
                num_points_in_pillar=8,
                return_intermediate=False,
                transformerlayers=dict(
                    type='VoxFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='DeformSelfAttention',
                            embed_dims=128,
                            num_levels=1,
                            num_points=8)
                    ],
                    ffn_cfgs=dict(
                        type='FFN',
                        embed_dims=128,
                        feedforward_channels=1024,
                        num_fcs=2,
                        ffn_drop=0.0,
                        act_cfg=dict(type='ReLU', inplace=True)),
                    feedforward_channels=256,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm')))),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=64,
            row_num_embed=512,
            col_num_embed=512)),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 0.2],
            point_cloud_range=[0, -25.6, -2.0, 51.2, 25.6, 4.4],
            out_size_factor=4)))
dataset_type = 'SemanticKittiDatasetStage2'
data_root = './kitti/'
file_client_args = dict(backend='disk')
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=4,
    train=dict(
        type='SemanticKittiDatasetStage2',
        split='train',
        test_mode=False,
        data_root='./kitti/',
        preprocess_root='./kitti/dataset',
        eval_range=51.2,
        depthmodel='msnet3d',
        nsweep=10,
        temporal=[],
        labels_tag='labels',
        query_tag='query_iou5203_pre7712_rec6153'),
    val=dict(
        type='SemanticKittiDatasetStage2',
        split='val',
        test_mode=True,
        data_root='./kitti/',
        preprocess_root='./kitti/dataset',
        eval_range=51.2,
        depthmodel='msnet3d',
        nsweep=10,
        temporal=[],
        labels_tag='labels',
        query_tag='query_iou5203_pre7712_rec6153'),
    test=dict(
        type='SemanticKittiDatasetStage2',
        split='val',
        test_mode=True,
        data_root='./kitti/',
        preprocess_root='./kitti/dataset',
        eval_range=51.2,
        depthmodel='msnet3d',
        nsweep=10,
        temporal=[],
        labels_tag='labels',
        query_tag='query_iou5203_pre7712_rec6153'),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
optimizer = dict(type='AdamW', lr=0.0002, weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 20
evaluation = dict(interval=1)
runner = dict(type='EpochBasedRunner', max_epochs=20)
gpu_ids = range(0, 1)

2024-05-31 09:58:20,344 - mmdet - INFO - Set random seed to 0, deterministic: False
2024-05-31 09:58:20,810 - mmdet - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'ckpts/resnet50-19c8e357.pth'}
2024-05-31 09:58:21,022 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
Name of parameter - Initialization information

pts_bbox_head.bev_embed.weight - torch.Size([262144, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.mask_embed.weight - torch.Size([1, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([512, 64]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([512, 64]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.level_embeds - torch.Size([4, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.cams_embeds - torch.Size([1, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.0.attentions.0.deformable_attention.sampling_offsets.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.0.attentions.0.deformable_attention.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.0.attentions.0.deformable_attention.attention_weights.weight - torch.Size([64, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.0.attentions.0.deformable_attention.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.0.attentions.0.deformable_attention.value_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.0.attentions.0.deformable_attention.value_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([256, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.0.norms.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.0.norms.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.0.norms.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.0.norms.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.1.attentions.0.deformable_attention.sampling_offsets.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.1.attentions.0.deformable_attention.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.1.attentions.0.deformable_attention.attention_weights.weight - torch.Size([64, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.1.attentions.0.deformable_attention.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.1.attentions.0.deformable_attention.value_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.1.attentions.0.deformable_attention.value_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([256, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.1.norms.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.1.norms.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.1.norms.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.1.norms.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.2.attentions.0.deformable_attention.sampling_offsets.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.2.attentions.0.deformable_attention.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.2.attentions.0.deformable_attention.attention_weights.weight - torch.Size([64, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.2.attentions.0.deformable_attention.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.2.attentions.0.deformable_attention.value_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.2.attentions.0.deformable_attention.value_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([256, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.2.norms.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.2.norms.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.2.norms.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.cross_transformer.encoder.layers.2.norms.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.level_embeds - torch.Size([4, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.cams_embeds - torch.Size([1, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([256, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.0.norms.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.0.norms.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.0.norms.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.0.norms.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([256, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.1.norms.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.1.norms.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.1.norms.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.self_transformer.encoder.layers.1.norms.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.header.mlp_head.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.header.mlp_head.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.header.mlp_head.1.weight - torch.Size([20, 128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

pts_bbox_head.header.mlp_head.1.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of VoxFormer  

img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.bn1.weight - torch.Size([64]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.bn1.bias - torch.Size([64]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from ckpts/resnet50-19c8e357.pth 

img_neck.lateral_convs.0.conv.weight - torch.Size([128, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.0.conv.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  

img_neck.fpn_convs.0.conv.weight - torch.Size([128, 128, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.0.conv.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VoxFormer  
2024-05-31 09:58:21,039 - mmdet - INFO - Model:
VoxFormer(
  (pts_bbox_head): VoxFormerHead(
    (bev_embed): Embedding(262144, 128)
    (mask_embed): Embedding(1, 128)
    (positional_encoding): LearnedPositionalEncoding(num_feats=64, row_num_embed=512, col_num_embed=512)
    (cross_transformer): PerceptionTransformer(
      (encoder): VoxFormerEncoder(
        (layers): ModuleList(
          (0): VoxFormerLayer(
            (attentions): ModuleList(
              (0): DeformCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=128, out_features=128, bias=True)
                  (attention_weights): Linear(in_features=128, out_features=64, bias=True)
                  (value_proj): Linear(in_features=128, out_features=128, bias=True)
                )
                (output_proj): Linear(in_features=128, out_features=128, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=128, out_features=256, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=256, out_features=128, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): VoxFormerLayer(
            (attentions): ModuleList(
              (0): DeformCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=128, out_features=128, bias=True)
                  (attention_weights): Linear(in_features=128, out_features=64, bias=True)
                  (value_proj): Linear(in_features=128, out_features=128, bias=True)
                )
                (output_proj): Linear(in_features=128, out_features=128, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=128, out_features=256, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=256, out_features=128, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): VoxFormerLayer(
            (attentions): ModuleList(
              (0): DeformCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=128, out_features=128, bias=True)
                  (attention_weights): Linear(in_features=128, out_features=64, bias=True)
                  (value_proj): Linear(in_features=128, out_features=128, bias=True)
                )
                (output_proj): Linear(in_features=128, out_features=128, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=128, out_features=256, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=256, out_features=128, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
    )
    (self_transformer): PerceptionTransformer(
      (encoder): VoxFormerEncoder(
        (layers): ModuleList(
          (0): VoxFormerLayer(
            (attentions): ModuleList(
              (0): DeformSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=128, out_features=128, bias=True)
                (output_proj): Linear(in_features=128, out_features=128, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=128, out_features=256, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=256, out_features=128, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): VoxFormerLayer(
            (attentions): ModuleList(
              (0): DeformSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=128, out_features=128, bias=True)
                (output_proj): Linear(in_features=128, out_features=128, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=128, out_features=256, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=256, out_features=128, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
    )
    (header): Header(
      (mlp_head): Sequential(
        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=128, out_features=20, bias=True)
      )
      (up_scale_2): Upsample(scale_factor=2.0, mode=trilinear)
    )
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg={'type': 'Pretrained', 'checkpoint': 'ckpts/resnet50-19c8e357.pth'}
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
)
